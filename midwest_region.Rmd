---
title: "midwest, southwest regions"
author: "Teagan Norrgard, trn8cwf"
date: "10/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud) 
library(textdata)
setwd("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab")
save.image("tidytext.RData")
library(ggplot2)
```



#### First I started by reading in all of the text files. This region is the mid-west, so I wanted to choose at least 1 article from each state to get a good representation. The states included in this region are North and South Dakota, Kansas, Nebraska, Minnesota, Iowa, Missouri, Wisconsin, Illinois, Indiana, Michigan, and Ohio.

```{r, warning=FALSE, message=FALSE, echo=FALSE}

nd1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/nd1.txt")
nd1 <- tibble(nd1)
nd2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/nd2.txt")
nd2 <- tibble(nd2)
illinois <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/illinois.txt")
illinois <- tibble(illinois)
indiana <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/indiana.txt")
indiana <- tibble(indiana)
kansas <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/kansas.txt")
kansas <- tibble(kansas)
wisconsin <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/wisconsin.txt")
wisconsin <- tibble(wisconsin)
sd <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/sd.txt")
sd <- tibble(sd)
michigan1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/michigan1.txt")
michigan1 <- tibble(michigan1)
michigan2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/michigan2.txt")
michigan2 <- tibble(michigan2)
minnesota <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/minnesota.txt")
minnesota <- tibble(minnesota)
mo1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/mo1.txt")
mo1 <- tibble(mo1)
mo2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/mo2.txt")
mo2 <- tibble(mo2)
nebraska <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/nebraska.txt")
nebraska <- tibble(nebraska)
iowa <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/iowa.txt")
iowa <- tibble(iowa)
ohio <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/ohio.txt")
ohio <- tibble(ohio)
```



#### Next I made frequency dataframes for words in the article, excluding stop words.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
nd1 <- nd1 %>%
  unnest_tokens(word, nd1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
nd2 <- nd2 %>%
  unnest_tokens(word, nd2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

illinois <- illinois %>%
  unnest_tokens(word, illinois)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

indiana <- indiana %>%
  unnest_tokens(word, indiana)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

kansas <- kansas %>%
  unnest_tokens(word, kansas)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

wisconsin <- wisconsin %>%
  unnest_tokens(word, wisconsin)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

sd <- sd %>%
  unnest_tokens(word, sd)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

michigan1 <- michigan1 %>%
  unnest_tokens(word, michigan1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
michigan2 <- michigan2 %>%
  unnest_tokens(word, michigan2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

minnesota <- minnesota %>%
  unnest_tokens(word, minnesota)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

mo1 <- mo1 %>%
  unnest_tokens(word, mo1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
mo2 <- mo2 %>%
  unnest_tokens(word, mo2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

nebraska <- nebraska %>%
  unnest_tokens(word, nebraska)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

iowa <- iowa %>%
  unnest_tokens(word, iowa)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

ohio <- ohio %>%
  unnest_tokens(word, ohio)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
```


#### It was interesting to see the wide range of issues that were at the top of these frequency lists. Some talked a lot about politics and government, others farmland, forrests, and migration, and some talked about floods and other extreme weather.


#### Then I used afinn, nrc, and bing methods to get sentiment values for each word that appeared in the frequency tables.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
get_sentiments('afinn')
get_sentiments('nrc')
get_sentiments('bing')

nd1_afinn <- nd1 %>%
  inner_join(get_sentiments("afinn"))
nd1_nrc <- nd1 %>%
  inner_join(get_sentiments("nrc"))
nd1_bing <- nd1 %>%
  inner_join(get_sentiments("bing"))

nd2_afinn <- nd2 %>%
  inner_join(get_sentiments("afinn"))
nd2_nrc <- nd2 %>%
  inner_join(get_sentiments("nrc"))
nd2_bing <- nd2 %>%
  inner_join(get_sentiments("bing"))

illinois_afinn <- illinois %>%
  inner_join(get_sentiments("afinn"))
illinois_nrc <- illinois %>%
  inner_join(get_sentiments("nrc"))
illinois_bing <- illinois %>%
  inner_join(get_sentiments("bing"))

indiana_afinn <- indiana %>%
  inner_join(get_sentiments("afinn"))
indiana_nrc <- indiana %>%
  inner_join(get_sentiments("nrc"))
indiana_bing <- indiana %>%
  inner_join(get_sentiments("bing"))

iowa_afinn <- iowa %>%
  inner_join(get_sentiments("afinn"))
iowa_nrc <- iowa %>%
  inner_join(get_sentiments("nrc"))
iowa_bing <- iowa %>%
  inner_join(get_sentiments("bing"))

kansas_afinn <- kansas %>%
  inner_join(get_sentiments("afinn"))
kansas_nrc <- kansas %>%
  inner_join(get_sentiments("nrc"))
kansas_bing <- kansas %>%
  inner_join(get_sentiments("bing"))

michigan1_afinn <- michigan1 %>%
  inner_join(get_sentiments("afinn"))
michigan1_nrc <- michigan1 %>%
  inner_join(get_sentiments("nrc"))
michigan1_bing <- michigan1 %>%
  inner_join(get_sentiments("bing"))

michigan2_afinn <- michigan2 %>%
  inner_join(get_sentiments("afinn"))
michigan2_nrc <- michigan2 %>%
  inner_join(get_sentiments("nrc"))
michigan2_bing <- michigan2 %>%
  inner_join(get_sentiments("bing"))

minnesota_afinn <- minnesota %>%
  inner_join(get_sentiments("afinn"))
minnesota_nrc <- minnesota %>%
  inner_join(get_sentiments("nrc"))
minnesota_bing <- minnesota %>%
  inner_join(get_sentiments("bing"))

mo1_afinn <- mo1 %>%
  inner_join(get_sentiments("afinn"))
mo1_nrc <- mo1 %>%
  inner_join(get_sentiments("nrc"))
mo1_bing <- mo1 %>%
  inner_join(get_sentiments("bing"))

mo2_afinn <- mo2 %>%
  inner_join(get_sentiments("afinn"))
mo2_nrc <- mo2 %>%
  inner_join(get_sentiments("nrc"))
mo2_bing <- mo2 %>%
  inner_join(get_sentiments("bing"))

nebraska_afinn <- nebraska %>%
  inner_join(get_sentiments("afinn"))
nebraska_nrc <- nebraska %>%
  inner_join(get_sentiments("nrc"))
nebraska_bing <- nebraska %>%
  inner_join(get_sentiments("bing"))

ohio_afinn <- ohio %>%
  inner_join(get_sentiments("afinn"))
ohio_nrc <- ohio %>%
  inner_join(get_sentiments("nrc"))
ohio_bing <- ohio %>%
  inner_join(get_sentiments("bing"))

sd_afinn <- sd %>%
  inner_join(get_sentiments("afinn"))
sd_nrc <- sd %>%
  inner_join(get_sentiments("nrc"))
sd_bing <- sd %>%
  inner_join(get_sentiments("bing"))

wisconsin_afinn <- wisconsin %>%
  inner_join(get_sentiments("afinn"))
wisconsin_nrc <- wisconsin %>%
  inner_join(get_sentiments("nrc"))
wisconsin_bing <- wisconsin %>%
  inner_join(get_sentiments("bing"))
```



#### Then I made tables of these sentiment values.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
table(nd1_bing$sentiment)
table(nd2_bing$sentiment)
table(illinois_bing$sentiment)
table(indiana_bing$sentiment)
table(iowa_bing$sentiment)
table(kansas_bing$sentiment)
table(michigan1_bing$sentiment)
table(michigan2_bing$sentiment)
table(minnesota_bing$sentiment)
table(mo1_bing$sentiment)
table(mo2_bing$sentiment)
table(nebraska_bing$sentiment)
table(ohio_bing$sentiment)
table(sd_bing$sentiment)
table(wisconsin_bing$sentiment)

table(nd1_nrc$sentiment)
table(nd2_nrc$sentiment)
table(illinois_nrc$sentiment)
table(indiana_nrc$sentiment)
table(iowa_nrc$sentiment)
table(kansas_nrc$sentiment)
table(michigan1_nrc$sentiment)
table(michigan2_nrc$sentiment)
table(minnesota_nrc$sentiment)
table(mo1_nrc$sentiment)
table(mo2_nrc$sentiment)
table(nebraska_nrc$sentiment)
table(ohio_nrc$sentiment)
table(sd_nrc$sentiment)
table(wisconsin_nrc$sentiment)

table(nd1_afinn$value)
table(nd2_afinn$value)
table(illinois_afinn$value)
table(indiana_afinn$value)
table(iowa_afinn$value)
table(kansas_afinn$value)
table(michigan1_afinn$value)
table(michigan2_afinn$value)
table(minnesota_afinn$value)
table(mo1_afinn$value)
table(mo2_afinn$value)
table(nebraska_afinn$value)
table(ohio_afinn$value)
table(sd_afinn$value)
table(wisconsin_afinn$value)
```

#### For the bing tables, words were either given a positive or negative rating. Most of the articles I chose had slightly more negative values than positive ones. The only states with articles that had more positive word ratings were North and South Dakota, Indiana, and Michigan. And the only state with a heavy skew was Minnesota, with 35 negative and only 8 positive.

#### What surprised me the most about the afinn tables was Michigan. Both articles I chose from Michigan were left skewed with relatively high sentiment values compared to the other articles I chose. Another surprising point I found was that none of the words had a score higher than 3 or lower than 4. I thought that some of the more conservative states I chose from might have higher sentiment values, and the more liberal states would have much lower ones, but they almost all followed a pretty normal distribution. I saw the same pattern in the nrc tables as described with the afinn tables. I decided not to make histograms of the afinn tables since I was able to interpret from the tables themselves.



#### Then I made word clouds to look at frequency again.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(42)
ggplot(nd1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(nd2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(illinois[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(indiana[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(iowa[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(kansas[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(michigan1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(michigan2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(minnesota[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(mo1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(mo2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(nebraska[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(ohio[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(sd[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(wisconsin[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
```


#### I don't think I came to any more conclusions from the wordclouds I made than from just looking at the frequency tables. It was easier to see the difference in frequencies within one article given the differing size of the text, but they didn't help much to compare between articles.


#### Finally I made  the tf_idf dataframe.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
nd1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/nd1.txt"))
nd2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/nd2.txt"))
illinois_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/illinois.txt"))
indiana_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/indiana.txt"))
kansas_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/kansas.txt"))
wisconsin_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/wisconsin.txt"))
sd_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/sd.txt"))
michigan1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/michigan1.txt"))
michigan2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/michigan2.txt"))
minnesota_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/minnesota.txt"))
mo1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/mo1.txt"))
mo2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/mo2.txt"))
nebraska_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/nebraska.txt"))
iowa_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/iowa.txt"))
ohio_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/ohio.txt"))

data_prep <- function(x,y,z){
  i <- as_tibble(t(x))    ## transposing dataset
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

nd1_bag <- data_prep(nd1_raw,'V1','V3')
nd2_bag <- data_prep(nd2_raw,'V1','V3')
illinois_bag <- data_prep(illinois_raw,'V1','V3')
indiana_bag <- data_prep(indiana_raw,'V1','V3')
kansas_bag <- data_prep(kansas_raw,'V1','V3')
wisconsin_bag <- data_prep(wisconsin_raw,'V1','V3')
sd_bag <- data_prep(sd_raw,'V1','V3')
michigan1_bag <- data_prep(michigan1_raw,'V1','V3')
michigan2_bag <- data_prep(michigan2_raw,'V1','V3')
minnesota_bag <- data_prep(minnesota_raw,'V1','V3')
mo1_bag <- data_prep(mo1_raw,'V1','V3')
mo2_bag <- data_prep(mo2_raw,'V1','V3')
nebraska_bag <- data_prep(nebraska_raw,'V1','V3')
iowa_bag <- data_prep(iowa_raw,'V1','V3')
ohio_bag <- data_prep(ohio_raw,'V1','V3')

titles_midwest <- c("Climate change, logging collide", "Organic tackles 'existential threat' of climate change",
            "Climate change is real, and it requires us to act", "Bipartisan action, leadership on climate change",
            "Climate change could force 200 million to move", "K-State experts outline climate change effects on Kansas",
            "Climate change: call it what you will, farmers face new challenges.",
            "Climate change: when local is global.", "Grim forecast in Minnesota climate adaptation report",
            "On St. Louis visit, US secretary of agriculture talks climate change", 
            "Report: Climate change could increase migration", "Analysis says climate rules to slow growth",
            "Scientists offer new estimate for how people altered planet",
            "South Dakota Board of Education passes science standards",
            "WANTED: LEADERS WILLING TO FACE CLIMATE CHANGE")

tf_idf_text <- tibble(titles_midwest,text=t(tibble(nd1_bag, nd2_bag, illinois_bag, indiana_bag, iowa_bag, 
                                           kansas_bag, michigan1_bag, michigan2_bag, minnesota_bag,
                                           mo1_bag, mo2_bag, nebraska_bag, ohio_bag, sd_bag, wisconsin_bag,
                                           .name_repair = "universal")))

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(titles_midwest, word, sort = TRUE)


total_words <- word_count %>% 
  group_by(titles_midwest) %>% 
  summarize(total = sum(n))

midwest_words <- left_join(word_count, total_words)

midwest_words <- midwest_words %>%
  bind_tf_idf(word, titles_midwest, n)
```

#### Some words I found with the highest tf_idf value were "forest", "agriculture", "education", and "organic"




## Next is the articles for the south west region.

#### I started by reading in all of the articles I chose. This region only had 4 states in it, so I chose 4 articles from each state, and for each state I used the same newspaper. The states included in this region are Texas, Oklahoma, Arizona, and New Mexico.

```{r, warning=FALSE, message=FALSE, echo=FALSE}

texas1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas1.txt")
texas1<- tibble(texas1)
texas2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas2.txt")
texas2<- tibble(texas2)
texas3 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas3.txt")
texas3 <- tibble(texas3)
texas4 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas4.txt")
texas4 <- tibble(texas4)
oklahoma1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma1.txt")
oklahoma1 <- tibble(oklahoma1)
oklahoma2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma2.txt")
oklahoma2 <- tibble(oklahoma2)
oklahoma3 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma3.txt")
oklahoma3 <- tibble(oklahoma3)
oklahoma4 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma4.txt")
oklahoma4 <- tibble(oklahoma4)
arizona1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona1.txt")
arizona1 <- tibble(arizona1)
arizona2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona2.txt")
arizona2 <- tibble(arizona2)
arizona3 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona3.txt")
arizona3 <- tibble(arizona3)
arizona4 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona4.txt")
arizona4 <- tibble(arizona4)
nm1 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm1.txt")
nm1 <- tibble(nm1)
nm2 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm2.txt")
nm2 <- tibble(nm2)
nm3 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm3.txt")
nm3 <- tibble(nm3)
nm4 <- read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm4.txt")
nm4 <- tibble(nm4)
```



#### Next I made frequency tables of each article, excluding the stop words.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
texas1 <- texas1 %>%
  unnest_tokens(word, texas1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
texas2 <- texas2 %>%
  unnest_tokens(word, texas2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
texas3 <- texas3 %>%
  unnest_tokens(word, texas3)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
texas4 <- texas4 %>%
  unnest_tokens(word, texas4)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

oklahoma1 <- oklahoma1 %>%
  unnest_tokens(word, oklahoma1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
oklahoma2 <- oklahoma2 %>%
  unnest_tokens(word, oklahoma2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
oklahoma3 <- oklahoma3 %>%
  unnest_tokens(word, oklahoma3)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
oklahoma4 <- oklahoma4 %>%
  unnest_tokens(word, oklahoma4)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

arizona1 <- arizona1 %>%
  unnest_tokens(word, arizona1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
arizona2 <- arizona2 %>%
  unnest_tokens(word, arizona2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
arizona3 <- arizona3 %>%
  unnest_tokens(word, arizona3)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
arizona4 <- arizona4 %>%
  unnest_tokens(word, arizona4)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)

nm1 <- nm1 %>%
  unnest_tokens(word, nm1)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
nm2 <- nm2 %>%
  unnest_tokens(word, nm2)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
nm3 <- nm3 %>%
  unnest_tokens(word, nm3)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
nm4 <- nm4 %>%
  unnest_tokens(word, nm4)%>%
  anti_join(stop_words)%>% 
  count(word, sort=TRUE)
```


#### Most of these articles had high frequency words about the economy, and what seemed to economic impacts of climate change. Only a few had words about politics and government, emissions and chemicals, or weather. 


#### Then I used afinn, nrc, and bing to get sentiment values for each word in the frequency tables.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
get_sentiments('afinn')
get_sentiments('nrc')
get_sentiments('bing')

texas1_afinn <- texas1 %>%
  inner_join(get_sentiments("afinn"))
texas1_nrc <- texas1 %>%
  inner_join(get_sentiments("nrc"))
texas1_bing <- texas1 %>%
  inner_join(get_sentiments("bing"))

texas2_afinn <- texas2 %>%
  inner_join(get_sentiments("afinn"))
texas2_nrc <- texas2 %>%
  inner_join(get_sentiments("nrc"))
texas2_bing <- texas2 %>%
  inner_join(get_sentiments("bing"))

texas3_afinn <- texas3 %>%
  inner_join(get_sentiments("afinn"))
texas3_nrc <- texas3 %>%
  inner_join(get_sentiments("nrc"))
texas3_bing <- texas3 %>%
  inner_join(get_sentiments("bing"))

texas4_afinn <- texas4 %>%
  inner_join(get_sentiments("afinn"))
texas4_nrc <- texas4 %>%
  inner_join(get_sentiments("nrc"))
texas4_bing <- texas4 %>%
  inner_join(get_sentiments("bing"))

table(texas1_bing$sentiment)
table(texas2_bing$sentiment)
table(texas3_bing$sentiment)
table(texas4_bing$sentiment)

table(texas1_nrc$sentiment)
table(texas2_nrc$sentiment)
table(texas3_nrc$sentiment)
table(texas4_nrc$sentiment)

table(texas1_afinn$value)
table(texas2_afinn$value)
table(texas3_afinn$value)
table(texas4_afinn$value)
```


```{r, warning=FALSE, message=FALSE, echo=FALSE}
oklahoma1_afinn <- oklahoma1 %>%
  inner_join(get_sentiments("afinn"))
oklahoma1_nrc <- oklahoma1 %>%
  inner_join(get_sentiments("nrc"))
oklahoma1_bing <- oklahoma1 %>%
  inner_join(get_sentiments("bing"))

oklahoma2_afinn <- oklahoma2 %>%
  inner_join(get_sentiments("afinn"))
oklahoma2_nrc <- oklahoma2 %>%
  inner_join(get_sentiments("nrc"))
oklahoma2_bing <- oklahoma2 %>%
  inner_join(get_sentiments("bing"))

oklahoma3_afinn <- oklahoma3 %>%
  inner_join(get_sentiments("afinn"))
oklahoma3_nrc <- oklahoma3 %>%
  inner_join(get_sentiments("nrc"))
oklahoma3_bing <- oklahoma3 %>%
  inner_join(get_sentiments("bing"))

oklahoma4_afinn <- oklahoma4 %>%
  inner_join(get_sentiments("afinn"))
oklahoma4_nrc <- oklahoma4 %>%
  inner_join(get_sentiments("nrc"))
oklahoma4_bing <- oklahoma4 %>%
  inner_join(get_sentiments("bing"))

table(oklahoma1_bing$sentiment)
table(oklahoma2_bing$sentiment)
table(oklahoma3_bing$sentiment)
table(oklahoma4_bing$sentiment)

table(oklahoma1_nrc$sentiment)
table(oklahoma2_nrc$sentiment)
table(oklahoma3_nrc$sentiment)
table(oklahoma4_nrc$sentiment)

table(oklahoma1_afinn$value)
table(oklahoma2_afinn$value)
table(oklahoma3_afinn$value)
table(oklahoma4_afinn$value)
```


```{r, warning=FALSE, message=FALSE, echo=FALSE}
arizona1_afinn <- arizona1 %>%
  inner_join(get_sentiments("afinn"))
arizona1_nrc <- arizona1 %>%
  inner_join(get_sentiments("nrc"))
arizona1_bing <- arizona1 %>%
  inner_join(get_sentiments("bing"))

arizona2_afinn <- arizona2 %>%
  inner_join(get_sentiments("afinn"))
arizona2_nrc <- arizona2 %>%
  inner_join(get_sentiments("nrc"))
arizona2_bing <- arizona2 %>%
  inner_join(get_sentiments("bing"))

arizona3_afinn <- arizona3 %>%
  inner_join(get_sentiments("afinn"))
arizona3_nrc <- arizona3 %>%
  inner_join(get_sentiments("nrc"))
arizona3_bing <- arizona3 %>%
  inner_join(get_sentiments("bing"))

arizona4_afinn <- arizona4 %>%
  inner_join(get_sentiments("afinn"))
arizona4_nrc <- arizona4 %>%
  inner_join(get_sentiments("nrc"))
arizona4_bing <- arizona4 %>%
  inner_join(get_sentiments("bing"))

table(arizona1_bing$sentiment)
table(arizona2_bing$sentiment)
table(arizona3_bing$sentiment)
table(arizona4_bing$sentiment)

table(arizona1_nrc$sentiment)
table(arizona2_nrc$sentiment)
table(arizona3_nrc$sentiment)
table(arizona4_nrc$sentiment)

table(arizona1_afinn$value)
table(arizona2_afinn$value)
table(arizona3_afinn$value)
table(arizona4_afinn$value)
```


```{r, warning=FALSE, message=FALSE, echo=FALSE}
nm1_afinn <- nm1 %>%
  inner_join(get_sentiments("afinn"))
nm1_nrc <- nm1 %>%
  inner_join(get_sentiments("nrc"))
nm1_bing <- nm1 %>%
  inner_join(get_sentiments("bing"))

nm2_afinn <- nm2 %>%
  inner_join(get_sentiments("afinn"))
nm2_nrc <- nm2 %>%
  inner_join(get_sentiments("nrc"))
nm2_bing <- nm2 %>%
  inner_join(get_sentiments("bing"))

nm3_afinn <- nm3 %>%
  inner_join(get_sentiments("afinn"))
nm3_nrc <- nm3 %>%
  inner_join(get_sentiments("nrc"))
nm3_bing <- nm3 %>%
  inner_join(get_sentiments("bing"))

nm4_afinn <- nm4 %>%
  inner_join(get_sentiments("afinn"))
nm4_nrc <- nm4 %>%
  inner_join(get_sentiments("nrc"))
nm4_bing <- nm4 %>%
  inner_join(get_sentiments("bing"))

table(nm1_bing$sentiment)
table(nm2_bing$sentiment)
table(nm3_bing$sentiment)
table(nm4_bing$sentiment)

table(nm1_nrc$sentiment)
table(nm2_nrc$sentiment)
table(nm3_nrc$sentiment)
table(nm4_nrc$sentiment)

table(nm1_afinn$value)
table(nm2_afinn$value)
table(nm3_afinn$value)
table(nm4_afinn$value)
```


#### For these tables, I wanted to compare the trends within each state. In the afinn tables, I found Texas to be very right skewed, with much more values in the negative numbers than positive. Oklahoma was the only state with an article that had more positive numbers than negative, and the Arizona and New Mexico ones followed a normal distribution more or less. 

#### The bing tables showed about the same thing, except there was no article with more postive values than negative, so it is surpising that the one from Oklahoma as mentioned before had higher afinn scores.



#### Then I made histograms for the afinn values from each article. I think I was able to understand this data just from the table, and these plots were unnecessary for me.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(data = texas1_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("texas1 Sentiment Range")+
  theme_minimal()

ggplot(data = texas2_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("texas2 Sentiment Range")+
  theme_minimal()

ggplot(data = texas3_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("texas3 Sentiment Range")+
  theme_minimal()

ggplot(data = texas4_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("texas4 Sentiment Range")+
  theme_minimal()
```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(data = oklahoma1_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("oklahoma1 Sentiment Range")+
  theme_minimal()

ggplot(data = oklahoma2_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("oklahoma2 Sentiment Range")+
  theme_minimal()

ggplot(data = oklahoma3_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("oklahoma3 Sentiment Range")+
  theme_minimal()

ggplot(data = oklahoma4_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("oklahoma4 Sentiment Range")+
  theme_minimal()
```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(data = arizona1_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("arizona1 Sentiment Range")+
  theme_minimal()

ggplot(data = arizona2_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("arizona2 Sentiment Range")+
  theme_minimal()

ggplot(data = arizona3_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("arizona3 Sentiment Range")+
  theme_minimal()

ggplot(data = arizona4_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("arizona4 Sentiment Range")+
  theme_minimal()
```



```{r, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(data = nm1_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("nm1 Sentiment Range")+
  theme_minimal()

ggplot(data = nm2_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("nm2 Sentiment Range")+
  theme_minimal()

ggplot(data = nm3_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("nm3 Sentiment Range")+
  theme_minimal()

ggplot(data = nm4_afinn, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("nm4 Sentiment Range")+
  theme_minimal()
```



#### Then I made wordclouds for the frequency tables.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(42)
ggplot(texas1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(texas2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(texas3[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(texas4[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(oklahoma1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(oklahoma2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(oklahoma3[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(oklahoma4[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(arizona1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(arizona2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(arizona3[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(arizona4[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()

ggplot(nm1[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(nm2[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(nm3[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
ggplot(nm4[1:50,], aes(label = word, size = n)) +
  geom_text_wordcloud() + theme_minimal()
```


#### Like the histograms, I'm not sure how much these helped me, as it wasn't easy to compare each wordcloud to the other ones. But I could see any words in each article that appeared many more times than others, since they were printed in a bigger size. It was easier to understand the scale of each word with the wordcloud than the frequency table.



#### Finally I made the tf_idf dataframe.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
texas1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas1.txt"))
texas2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas2.txt"))
texas3_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas3.txt"))
texas4_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/texas4.txt"))
oklahoma1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma1.txt"))
oklahoma2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma2.txt"))
oklahoma3_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma3.txt"))
oklahoma4_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/oklahoma4.txt"))
arizona1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona1.txt"))
arizona2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona2.txt"))
arizona3_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona3.txt"))
arizona4_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/arizona4.txt"))
nm1_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm1.txt"))
nm2_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm2.txt"))
nm3_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm3.txt"))
nm4_raw <- as.tibble(read_lines("/Users/teagannorrgard/Fall2021/DS3001/TextMiningLab/south_region/nm4.txt"))


data_prep <- function(x,y,z){
  i <- as_tibble(t(x))    ## transposing dataset
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

texas1_bag <- data_prep(texas1_raw,'V1','V3')
texas2_bag <- data_prep(texas2_raw,'V1','V3')
texas3_bag <- data_prep(texas3_raw,'V1','V3')
texas4_bag <- data_prep(texas4_raw,'V1','V3')
oklahoma1_bag <- data_prep(oklahoma1_raw,'V1','V3')
oklahoma2_bag <- data_prep(oklahoma2_raw,'V1','V3')
oklahoma3_bag <- data_prep(oklahoma3_raw,'V1','V3')
oklahoma4_bag <- data_prep(oklahoma4_raw,'V1','V3')
arizona1_bag <- data_prep(arizona1_raw,'V1','V3')
arizona2_bag <- data_prep(arizona2_raw,'V1','V3')
arizona3_bag <- data_prep(arizona3_raw,'V1','V3')
arizona4_bag <- data_prep(arizona4_raw,'V1','V3')
nm1_bag <- data_prep(nm1_raw,'V1','V3')
nm2_bag <- data_prep(nm2_raw,'V1','V3')
nm3_bag <- data_prep(nm3_raw,'V1','V3')
nm4_bag <- data_prep(nm4_raw,'V1','V3')


titles_south <- c("Is Climate Change Denial Thawing in Texas?",
            "Industrial Evolution", "High and Dry", "POWER SHIFT", "Climate change talk subtle in state",
            "Oklahoma faith community to discuss climate and environment", 
            "Strong tornadoes not linked to climate change, experts say", "Hot or cold?",
            "Action needed on climate change to protect tourism, agriculture", 
            "Why global climate change solutions shouldn‘t be government-led",
            "Climate changes affect everyone, time to act",
            "Climate-change science proven and certain as dangers to planet grow",
            "WHAT WOULD DARWIN DO?", "Climate change: Just deal with it",
            "Fight climate change - for justice and health", "Legislature must act boldly on climate change")

tf_idf_text <- tibble(titles_south,text=t(tibble(texas1_bag,texas2_bag,texas3_bag,texas4_bag,
                                           oklahoma1_bag, oklahoma2_bag, oklahoma3_bag, oklahoma4_bag,
                                           arizona1_bag, arizona2_bag, arizona3_bag, arizona4_bag,
                                           nm1_bag, nm2_bag, nm3_bag, nm4_bag,
                                           .name_repair = "universal")))

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(titles_south, word, sort = TRUE)


total_words <- word_count %>% 
  group_by(titles_south) %>% 
  summarize(total = sum(n))

south_words <- left_join(word_count, total_words)


south_words <- south_words %>%
  bind_tf_idf(word, titles_south, n)
```


#### Some words I found with the highest tf_idf values were "water", "plant", "energy", and "oil". I think this is an interesting difference between words like "education" and "agriculture" from the midwest region I looked at. It seems like the south region is more concerned with the root of the problem and possible solutions, where the midwest region is concerned with affects climate change could have.

